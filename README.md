# Enron-poi-identifier: Building a classifier for identifying persons of interest in the Enron fraud investigation.

This is a repository containing my work on building a "person of interest" identifier for the Enron fraud investigation using machine learning. This identifier is the final project of Udacity's Introduction to Machine Learning course. Starter code for the project, along with a code for the mini-projects for this course, was obtained from the github page of the course: https://github.com/udacity/ud120-projects. I've only included the dataset used for the final project from that page, if you want to work on this project on your own or build more advanced features using the full email text data, you will need to clone the files there.

The goal of the final project is to build a classifier that can predict whether an Enron employee should have been a person of interest in the fraud investigation.  My personal goal from this project is not to answer any specific question about the dataset but rather to practice building effective machine learning models in a practical way, to learn how to deal with messy data and to practice presenting my analysis in a clear manner. This project is very good for this, as the data has a lot of problems but is still a good toy model to practice machine learning tools available via sklearn and other packages. As such, I have made two notebooks, one for qualitative data analysis and visualization and the other for building a machine learning model.

In the first notebook, titled Exploratory Analysis, I visualize the data and study the relationship of the features with the poi (person of interest) label. I also normalize the email counts for each employee to create a better feature than the full email counts. One big issue with the data is that there are a large number of missing feature values. I studied two different ways to deal with these values: first, by imputing a value of 0 for all of them and second, by imputing the median of the feature value. Neither seemed to give particularly different separation between the two classes.

A related issue with the data is that the classes don't separate very well in the feature space. This seems somewhat expected, the Udacity guidelines for the course simply want a model that has precision and recall over 0.3. Frankly, any such model seems pretty useless to me, but tuning the models and trying to improve performance has forced me to learn about a lot of different issues that can come up with data and how I can deal with them. Using stratified shuffle splits to create better evaluation metrics on limited data and learning how to deal with imbalanced class problems has been very informative.

In the second notebook, titled Poi Identifier, I built some machine learning models on the data. I started with some feature selection via decision tree feature importances and then trained a decision tree classifier. This classifier doesn't give incredible results, as the data is not particularly seperable, but we hit the targets expected by Udacity. We also develop a scoring metric that helps deal with issue of just not having enough data. 

**Future Work:** 

The next steps I want to take in the project is to deal with problems of overfitting and imbalanced classes. Overfitting is a big issue since the dataset is very small. We have to use the full dataset to train since we need to use repeated sampling via stratified shuffle splits to get any reasonable metric, and having a test set held out won't help, as the test set will be small enough that the random chance of a bad or good split will skew precision and recall metrics. So, in the project so far, I used a jerry rigged version of nested cross-validation to try and measure the model strength before fitting it on the whole dataset. This doesn't completely alleviate the overfitting issue so the next step I wish to take is to use ensembles and train a Random Forest Classifier.

The other big issue with this project is the fact that the poi class is much smaller than the non_poi class. To deal with this, I want to try SMOTE oversampling, mostly because I want to learn how to use this algorithm. I'm not sure how impactful it will be, but it is something worth trying.

Ultimately, my goal will be to try and push the precision and recall of the model above 50%. I don't think a significantly better score is achievable due to the lack of separation of the classes.
